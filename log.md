# 100 Days of Machine Learning Code - LOG
## Day 1: 16 July 2018 

**Today's Progress** : Implemented a linear regression model to predict housing prices with scikit and learnt about fine-tuning it via grid search, randomized search and ensemble methods. Followed tutorials from the textbook: Hands on Machine learning and used various medium articles to supplement theories that was hard to understand

**Thoughts**: It was great to apply linear regression a concept that I had learnt in previous math classes to machine learning and come up with a real-life solution to a problem. I was surprised by how simple it was to actually implement the model and learnt that the real challenge was in cleaning, normalizing and scaling the dataset. 

**Link of work**: https://github.com/tanster1234/100daysofML/blob/master/Work/Housing.ipynb

## Day 2: 17 July 2018 

**Today's Progress**: Learned to train a binary classifier using stochastic gradient descent(SGD) from sci-kit Learn's SGDClassifier Class for the MNIST dataset. Then used various performance measures such as precision and recall graphs and a ROC curve to see how precise the classifier is and how well it can recall. After learning binary classification went onwards to multi-label classification, using K nearest-neighbor algorithm. Managed to classify the dataset at 97% accuracy by using gride search for KNeighborsClassifier. 

**Thoughts**: Implementing SGD and KNearestNeighbors was pretty easy with very little lines of code needed, what was much harder though was how to measure the performance of the classifier using cross-validation and precision/recall. Learning about the precision/recall tradeof was pretty enlightening and I quite enjoyed it. 

**Link of work**: https://github.com/tanster1234/100daysofML/blob/master/Work/Classification%20(MNIST).ipynb

## Day 3: 18 July 2018

**Today's Progress**: Learned more deeply about linear regression and how to find optimal parameters using the normal equation and by using various gradient descent methods such as batch gradient descent and stochastic gradient descent. Then went on to learn about polynomial regression and looked at new types of Regularized linear models such as Ridge regression, Lasso regression and elastic net. 

**Thoughts**: This was quite math heavy as it required a deeper understanding of how the statistical princples and equations relate, didn't work on a dataset for today but rather just experimented with various methods of finding the best model. 

**Link of work**: https://github.com/tanster1234/100daysofML/blob/master/Work/Training%20Linear%20Models.ipynb
